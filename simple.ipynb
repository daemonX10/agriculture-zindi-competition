{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc14955",
   "metadata": {},
   "source": [
    "# Zindi Agriculture Data Processing Pipeline\n",
    "\n",
    "This notebook organizes the Zindi Agriculture competition data into three different complexity levels:\n",
    "\n",
    "1. **Basic data**: Original data organized in a 'basic_data' folder\n",
    "2. **Intermediate data**: Enhanced with new features and dynamic analysis\n",
    "3. **Advanced data**: Most sophisticated features (to be implemented later)\n",
    "\n",
    "The CÃ´te d'Ivoire Byte-Sized Agriculture Challenge requires classifying agricultural fields into three crop types (Cocoa, Palm, and Rubber) using Sentinel-2 satellite imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49476cb9",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ca1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy matplotlib seaborn geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb71a9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeopandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f1ef81",
   "metadata": {},
   "source": [
    "## Set Up Directory Structure\n",
    "\n",
    "Create directories for basic, intermediate, and advanced data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798bf541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "base_dir = Path('.')\n",
    "basic_dir = base_dir / 'basic_data'\n",
    "intermediate_dir = base_dir / 'intermediate_data'\n",
    "advanced_dir = base_dir / 'advanced_data'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [basic_dir, intermediate_dir, advanced_dir]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "print(f\"Created directory structure in {base_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed402b8",
   "metadata": {},
   "source": [
    "## Load and Explore Basic Data\n",
    "\n",
    "Let's load the original data files and explore their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample_train.csv\n",
    "sample_train_path = 'data/sample_train.csv'\n",
    "sample_train = pd.read_csv(sample_train_path)\n",
    "\n",
    "# Load train.geojson\n",
    "train_geo_path = 'data/train.geojson'\n",
    "train_geo = gpd.read_file(train_geo_path)\n",
    "\n",
    "# Load test.geojson\n",
    "test_geo_path = 'data/test.geojson'\n",
    "test_geo = gpd.read_file(test_geo_path)\n",
    "\n",
    "# Load submission template\n",
    "submission_path = 'data/SampleSubmission.csv'\n",
    "submission_template = pd.read_csv(submission_path)\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"Sample Train Data Shape:\", sample_train.shape)\n",
    "print(\"Train GeoJSON Data Shape:\", train_geo.shape)\n",
    "print(\"Test GeoJSON Data Shape:\", test_geo.shape)\n",
    "print(\"Submission Template Shape:\", submission_template.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4095f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine sample_train.csv\n",
    "print(\"\\nSample Train Data:\")\n",
    "display(sample_train.head())\n",
    "\n",
    "# Examine class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "display(sample_train['Target'].value_counts())\n",
    "\n",
    "# Check the mapping of Target to class\n",
    "print(\"\\nTarget to Class Mapping:\")\n",
    "display(sample_train[['Target', 'class']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine geometry data\n",
    "print(\"\\nTrain GeoJSON Sample:\")\n",
    "display(train_geo.head(2))\n",
    "\n",
    "print(\"\\nTest GeoJSON Sample:\")\n",
    "display(test_geo.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b43aed",
   "metadata": {},
   "source": [
    "## Basic Data Processing\n",
    "\n",
    "Process and save the basic data version to the 'basic_data' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b97a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process IDs in sample_train\n",
    "def extract_id_base(id_str):\n",
    "    # Extract base ID (without month suffix)\n",
    "    parts = id_str.split('_')\n",
    "    if len(parts) >= 3:  # Format: ID_XXXXX_Month\n",
    "        return f\"ID_{parts[1]}\"\n",
    "    return id_str\n",
    "\n",
    "# Add base_id column to sample_train\n",
    "sample_train['base_id'] = sample_train['ID'].apply(extract_id_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the sample_train with train_geo to get geometries\n",
    "# First, extract base IDs for train_geo\n",
    "train_geo['base_id'] = train_geo['ID'].apply(lambda x: x)\n",
    "\n",
    "# Join sample_train with train_geo\n",
    "# We'll join on base_id because sample_train IDs have month suffixes\n",
    "# Convert to GeoDataFrame for better spatial operations\n",
    "basic_train = pd.merge(\n",
    "    sample_train,\n",
    "    train_geo[['ID', 'base_id', 'geometry']],\n",
    "    on='base_id',\n",
    "    how='left',\n",
    "    suffixes=('', '_geo')\n",
    ")\n",
    "\n",
    "# Check if we have missing geometries\n",
    "missing_geo = basic_train['geometry'].isna().sum()\n",
    "print(f\"Records with missing geometries: {missing_geo} out of {len(basic_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the basic datasets to basic_data folder\n",
    "basic_train.to_csv(basic_dir / 'basic_train.csv', index=False)\n",
    "\n",
    "# Also save the test and submission templates\n",
    "test_geo.to_file(basic_dir / 'basic_test.geojson', driver='GeoJSON')\n",
    "submission_template.to_csv(basic_dir / 'basic_submission_template.csv', index=False)\n",
    "\n",
    "print(f\"Basic data saved to {basic_dir}\")\n",
    "\n",
    "# Save a summary of the basic data\n",
    "with open(basic_dir / 'basic_data_summary.txt', 'w') as f:\n",
    "    f.write(f\"Sample Train Shape: {sample_train.shape}\\n\")\n",
    "    f.write(f\"Train GeoJSON Shape: {train_geo.shape}\\n\")\n",
    "    f.write(f\"Test GeoJSON Shape: {test_geo.shape}\\n\")\n",
    "    f.write(f\"Class Distribution:\\n{sample_train['Target'].value_counts().to_string()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a2a7a9",
   "metadata": {},
   "source": [
    "## Visualize Basic Data\n",
    "\n",
    "Create some visualizations of the basic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a66fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Target', data=sample_train, palette='viridis')\n",
    "plt.title('Distribution of Crop Types in Training Data')\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Crop Type')\n",
    "plt.tight_layout()\n",
    "plt.savefig(basic_dir / 'class_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a few sample geometries by crop type\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "crop_types = ['Cocoa', 'Palm', 'Rubber']\n",
    "colors = ['green', 'orange', 'blue']\n",
    "\n",
    "for i, crop in enumerate(crop_types):\n",
    "    # Get sample geometries for this crop type\n",
    "    crop_ids = sample_train[sample_train['Target'] == crop]['base_id'].unique()[:5]\n",
    "    crop_geo = train_geo[train_geo['base_id'].isin(crop_ids)]\n",
    "    \n",
    "    if not crop_geo.empty:\n",
    "        crop_geo.plot(ax=ax[i], color=colors[i], alpha=0.7)\n",
    "        ax[i].set_title(f'{crop} Field Samples')\n",
    "        ax[i].set_axis_off()\n",
    "    else:\n",
    "        ax[i].text(0.5, 0.5, f\"No geometries found for {crop}\", \n",
    "                 horizontalalignment='center', verticalalignment='center')\n",
    "        ax[i].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basic_dir / 'sample_geometries.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41328234",
   "metadata": {},
   "source": [
    "## Intermediate Data Processing\n",
    "\n",
    "Create an enhanced version of the data with additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the basic_train for enhancement\n",
    "intermediate_train = basic_train.copy()\n",
    "\n",
    "# Add geometry features\n",
    "# Convert to GeoDataFrame to use spatial operations\n",
    "gdf = gpd.GeoDataFrame(intermediate_train, geometry='geometry')\n",
    "\n",
    "# Calculate basic geometric features\n",
    "gdf['area'] = gdf['geometry'].apply(lambda geom: geom.area if geom else None)\n",
    "gdf['perimeter'] = gdf['geometry'].apply(lambda geom: geom.length if geom else None)\n",
    "gdf['compactness'] = gdf.apply(lambda row: 4 * np.pi * row['area'] / (row['perimeter'] ** 2) \n",
    "                               if row['perimeter'] and row['area'] else None, axis=1)\n",
    "\n",
    "# Calculate centroid coordinates\n",
    "gdf['centroid_x'] = gdf['geometry'].apply(lambda geom: geom.centroid.x if geom else None)\n",
    "gdf['centroid_y'] = gdf['geometry'].apply(lambda geom: geom.centroid.y if geom else None)\n",
    "\n",
    "# Display enhanced dataset\n",
    "print(\"Intermediate dataset with geometric features:\")\n",
    "display(gdf[['ID', 'Target', 'area', 'perimeter', 'compactness', 'centroid_x', 'centroid_y']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text features from the ID and file path\n",
    "gdf['path_length'] = gdf['tifPath'].apply(len)\n",
    "\n",
    "# Extract image info from path\n",
    "def extract_image_info(path):\n",
    "    if pd.isna(path):\n",
    "        return pd.Series({'image_crop': None, 'image_id': None, 'image_year': None, 'image_month': None})\n",
    "    \n",
    "    # Example path: /content/drive/MyDrive/all_images_s2/s2_Rubber_ID_Mrbi2k_2024_01.tif\n",
    "    try:\n",
    "        filename = os.path.basename(path)\n",
    "        parts = filename.split('_')\n",
    "        \n",
    "        if len(parts) >= 6:  # Format: s2_Crop_ID_XXXXX_YYYY_MM.tif\n",
    "            return pd.Series({\n",
    "                'image_crop': parts[1],\n",
    "                'image_id': parts[3],\n",
    "                'image_year': parts[4],\n",
    "                'image_month': parts[5].split('.')[0]\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing path {path}: {e}\")\n",
    "    \n",
    "    return pd.Series({'image_crop': None, 'image_id': None, 'image_year': None, 'image_month': None})\n",
    "\n",
    "# Extract features from the path\n",
    "path_features = gdf['tifPath'].apply(extract_image_info)\n",
    "gdf = pd.concat([gdf, path_features], axis=1)\n",
    "\n",
    "# Display the enhanced features\n",
    "print(\"\\nIntermediate dataset with path extraction features:\")\n",
    "display(gdf[['ID', 'Target', 'image_crop', 'image_id', 'image_year', 'image_month']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7e7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional derived features\n",
    "\n",
    "# Calculate the area to perimeter ratio\n",
    "gdf['area_perimeter_ratio'] = gdf['area'] / gdf['perimeter']\n",
    "\n",
    "# Create a categorical feature for month\n",
    "gdf['month_cat'] = gdf['month']\n",
    "\n",
    "# Create a one-hot encoding for month\n",
    "month_dummies = pd.get_dummies(gdf['month_cat'], prefix='month')\n",
    "gdf = pd.concat([gdf, month_dummies], axis=1)\n",
    "\n",
    "# Display the final intermediate dataset\n",
    "print(\"\\nFinal Intermediate dataset:\")\n",
    "display(gdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the geometry features by crop type\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(x='Target', y='area', data=gdf)\n",
    "plt.title('Area by Crop Type')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(x='Target', y='perimeter', data=gdf)\n",
    "plt.title('Perimeter by Crop Type')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(x='Target', y='compactness', data=gdf)\n",
    "plt.title('Compactness by Crop Type')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(intermediate_dir / 'geometric_features_by_crop.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the intermediate dataset\n",
    "gdf.to_csv(intermediate_dir / 'intermediate_train.csv', index=False)\n",
    "\n",
    "# Also save as GeoJSON for spatial analysis\n",
    "gdf.to_file(intermediate_dir / 'intermediate_train.geojson', driver='GeoJSON')\n",
    "\n",
    "# Enhance test data with the same features\n",
    "intermediate_test = test_geo.copy()\n",
    "intermediate_test['area'] = intermediate_test['geometry'].apply(lambda geom: geom.area if geom else None)\n",
    "intermediate_test['perimeter'] = intermediate_test['geometry'].apply(lambda geom: geom.length if geom else None)\n",
    "intermediate_test['compactness'] = intermediate_test.apply(\n",
    "    lambda row: 4 * np.pi * row['area'] / (row['perimeter'] ** 2) if row['perimeter'] and row['area'] else None, \n",
    "    axis=1\n",
    ")\n",
    "intermediate_test['centroid_x'] = intermediate_test['geometry'].apply(lambda geom: geom.centroid.x if geom else None)\n",
    "intermediate_test['centroid_y'] = intermediate_test['geometry'].apply(lambda geom: geom.centroid.y if geom else None)\n",
    "intermediate_test['area_perimeter_ratio'] = intermediate_test['area'] / intermediate_test['perimeter']\n",
    "\n",
    "# Save the intermediate test data\n",
    "intermediate_test.to_file(intermediate_dir / 'intermediate_test.geojson', driver='GeoJSON')\n",
    "\n",
    "print(f\"Intermediate data saved to {intermediate_dir}\")\n",
    "\n",
    "# Save a summary of intermediate features\n",
    "with open(intermediate_dir / 'intermediate_features_summary.txt', 'w') as f:\n",
    "    f.write(\"Geometric Features Added:\\n\")\n",
    "    f.write(\"- area: Area of the field polygon\\n\")\n",
    "    f.write(\"- perimeter: Perimeter of the field polygon\\n\")\n",
    "    f.write(\"- compactness: 4Ï * area / perimeterÂ² (measure of how circular the field is)\\n\")\n",
    "    f.write(\"- centroid_x, centroid_y: Coordinates of the field centroid\\n\")\n",
    "    f.write(\"- area_perimeter_ratio: Area divided by perimeter\\n\\n\")\n",
    "    \n",
    "    f.write(\"Path Features Added:\\n\")\n",
    "    f.write(\"- path_length: Length of the image path string\\n\")\n",
    "    f.write(\"- image_crop: Crop type extracted from the image filename\\n\")\n",
    "    f.write(\"- image_id: ID extracted from the image filename\\n\")\n",
    "    f.write(\"- image_year: Year extracted from the image filename\\n\")\n",
    "    f.write(\"- image_month: Month extracted from the image filename\\n\\n\")\n",
    "    \n",
    "    f.write(\"One-hot Encoded Features:\\n\")\n",
    "    f.write(\"- month_Jan: Flag for January\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe07431",
   "metadata": {},
   "source": [
    "## Advanced Data Processing (Placeholder)\n",
    "\n",
    "This section is a placeholder for more advanced data processing that will be implemented later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f8f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a placeholder for advanced data processing\n",
    "with open(advanced_dir / 'advanced_processing_notes.txt', 'w') as f:\n",
    "    f.write(\"# Advanced Data Processing - To Be Implemented\\n\\n\")\n",
    "    f.write(\"This section will include more advanced data processing such as:\\n\\n\")\n",
    "    f.write(\"1. Extraction of spectral indices from Sentinel-2 satellite imagery\\n\")\n",
    "    f.write(\"2. Texture analysis of satellite imagery\\n\")\n",
    "    f.write(\"3. Time series analysis for multi-temporal imagery\\n\")\n",
    "    f.write(\"4. Context features (proximity to other fields, roads, water bodies)\\n\")\n",
    "    f.write(\"5. Deep learning feature extraction using pre-trained models\\n\")\n",
    "    f.write(\"6. Climate and soil data integration\\n\")\n",
    "    f.write(\"7. Advanced geometric features (fractal dimension, shape complexity)\\n\")\n",
    "\n",
    "print(f\"Advanced data placeholder created in {advanced_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260af53",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has prepared three levels of data complexity for the Zindi Agriculture competition:\n",
    "\n",
    "1. **Basic data**: Original dataset organized with minimal processing\n",
    "2. **Intermediate data**: Enhanced with geometric features and additional derived features\n",
    "3. **Advanced data**: Placeholder for more sophisticated features to be implemented later\n",
    "\n",
    "Each level is saved in its respective directory for further use in modeling and analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
